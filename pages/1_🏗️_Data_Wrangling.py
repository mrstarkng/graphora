import streamlit as st
import pandas as pd
from utils.data_loader import load_data # Import h√†m load_data

st.set_page_config(page_title="Data Wrangling", layout="wide") # C·∫•u h√¨nh ri√™ng cho trang n√†y

st.title("üèóÔ∏è Data Wrangling")

# T·∫£i d·ªØ li·ªáu b·∫±ng h√†m ƒë√£ import
# data = load_data("train_and_val.csv") # File g·ªëc ƒë·ªÉ x·ª≠ l√Ω
# raw_display_data = load_data("training.xlsx") # File kh√°c ƒë·ªÉ hi·ªÉn th·ªã raw

# --- B·∫ÆT ƒê·∫¶U CODE C·ª¶A PH·∫¶N data_wrangling T·ª™ FILE G·ªêC ---

# V√≠ d·ª•: gi·∫£ s·ª≠ file ch√≠nh l√† 'train_and_val.csv'
data = load_data("train_and_val.csv")

if data is not None:
    st.write("Performing data wrangling steps...")

    # Drop Unnecessary Columns
    if "Region" in data.columns:
        data_processed = data.drop(columns=["Region"]) # T·∫°o b·∫£n copy ƒë·ªÉ x·ª≠ l√Ω
    else:
        data_processed = data.copy()

    # Handle Missing Values
    if 'Model Year' in data_processed.columns:
        median_year = data_processed['Model Year'].median()
        data_processed['Model Year'].fillna(median_year, inplace=True)
        # Ch·ªâ chuy·ªÉn sang int n·∫øu kh√¥ng c√≥ NaN sau khi fill
        if not data_processed['Model Year'].isnull().any():
             data_processed['Model Year'] = data_processed['Model Year'].astype(int)
        else:
             st.warning("Could not convert 'Model Year' to int due to remaining NaNs.")
    else:
        st.warning("'Model Year' column not found.")

    # Convert 'GVWR Class' to Numeric
    if "GVWR Class" in data_processed.columns:
        data_processed["GVWR Class"] = data_processed["GVWR Class"].replace({"Not Applicable": -1, "Unknown": -1})
        # Chuy·ªÉn ƒë·ªïi an to√†n h∆°n
        data_processed["GVWR Class"] = pd.to_numeric(data_processed["GVWR Class"], errors='coerce')
        # C√≥ th·ªÉ fillna sau khi coerce n·∫øu c·∫ßn
        # data_processed["GVWR Class"].fillna(-1, inplace=True) # V√≠ d·ª• fill NaN c√≤n l·∫°i
    else:
        st.warning("'GVWR Class' column not found.")


    # One-Hot Encoding & Ordinal Encoding
    if "Number of Vehicles Registered at the Same Address" in data_processed.columns:
        ordinal_mapping = {'1': 1, '2': 2, '3': 3, '‚â•4': 4, 'Unknown': -1}
        data_processed["Number of Vehicles Registered at the Same Address"] = data_processed["Number of Vehicles Registered at the Same Address"].map(ordinal_mapping)
         # Fillna cho c√°c gi√° tr·ªã kh√¥ng c√≥ trong mapping
        # data_processed["Number of Vehicles Registered at the Same Address"].fillna(-1, inplace=True)
    else:
        st.warning("'Number of Vehicles Registered...' column not found.")

    # X√°c ƒë·ªãnh c√°c c·ªôt ƒë·ªÉ one-hot encode m·ªôt c√°ch an to√†n
    cols_to_encode = ["Vehicle Category", "Fuel Type", "Fuel Technology", "Electric Mile Range"]
    existing_cols_to_encode = [col for col in cols_to_encode if col in data_processed.columns]
    if existing_cols_to_encode:
         data_processed = pd.get_dummies(data_processed, columns=existing_cols_to_encode, dummy_na=True) # dummy_na=True ƒë·ªÉ x·ª≠ l√Ω NaN n·∫øu c√≥
    else:
         st.warning("No columns found for One-Hot Encoding.")


    # Convert Date Column
    if "Date" in data_processed.columns and "Model Year" in data_processed.columns:
        # X·ª≠ l√Ω NaN trong Date tr∆∞·ªõc khi chuy·ªÉn ƒë·ªïi
        median_date = data_processed['Date'].median() if pd.api.types.is_numeric_dtype(data_processed['Date']) else 2020 # Gi·∫£ s·ª≠ nƒÉm 2020 n·∫øu kh√¥ng ph·∫£i s·ªë
        data_processed['Date'].fillna(median_date, inplace=True)

        # Chuy·ªÉn ƒë·ªïi sang datetime v√† x·ª≠ l√Ω l·ªói
        data_processed['Date_temp'] = pd.to_datetime(data_processed['Date'], format='%Y', errors='coerce')
        valid_dates = data_processed['Date_temp'].notna()

        # Ch·ªâ t√≠nh to√°n Vehicle Age n·∫øu Date v√† Model Year h·ª£p l·ªá
        if 'Model Year' in data_processed.columns and data_processed['Model Year'].notna().all():
            data_processed.loc[valid_dates, "Vehicle Age"] = data_processed.loc[valid_dates, 'Date_temp'].dt.year - data_processed.loc[valid_dates, "Model Year"]
            # C√≥ th·ªÉ fillna cho Vehicle Age n·∫øu mu·ªën
            # data_processed["Vehicle Age"].fillna(data_processed["Vehicle Age"].median(), inplace=True)
        else:
            st.warning("Cannot calculate 'Vehicle Age' due to issues in 'Date' or 'Model Year'.")
        data_processed.drop(columns=['Date_temp'], inplace=True) # X√≥a c·ªôt t·∫°m

    else:
         st.warning("Missing 'Date' or 'Model Year' for 'Vehicle Age' calculation.")

    # --- C√°c b∆∞·ªõc hi·ªÉn th·ªã gi·∫£i th√≠ch code ---
    st.header("Step 1: Load and Display the Dataset")
    st.write("First, we load the dataset and display the first few rows to understand its structure.")
    code_step1 = """
import pandas as pd

# Load the dataset (adjust path as needed)
# data = pd.read_csv("data/train_and_val.csv")
# For display purposes, load the 'training.xlsx'
raw_display_data = pd.read_excel("data/training.xlsx")

# Display the first 5 rows
print(raw_display_data.head())
"""
    st.code(code_step1, language="python")
    st.write("**Explanation**: The dataset is loaded and the first 5 rows are displayed.")
    st.write("### üîç Raw Dataset Preview (from training.xlsx)")
    raw_display_data_loaded = load_data("training.xlsx")
    if raw_display_data_loaded is not None:
        st.dataframe(raw_display_data_loaded.head())
    else:
        st.warning("Could not load 'training.xlsx' for preview.")


    st.header("Step 2: Basic Information about the Dataset")
    # ... (code v√† gi·∫£i th√≠ch t∆∞∆°ng t·ª± nh∆∞ file g·ªëc) ...
    code_step2 = """
# Assuming 'data' holds the loaded dataframe
print(f"Number of rows: {data.shape[0]}, Number of columns: {data.shape[1]}")
data.info()
print("\\nMissing values:")
print(data.isnull().sum())
"""
    st.code(code_step2, language="python")
    st.write("**Explanation**: Check shape, data types, and missing values.")

    st.header("Step 3: Handle Missing Values")
    # ... (code v√† gi·∫£i th√≠ch t∆∞∆°ng t·ª± nh∆∞ file g·ªëc) ...
    code_step3 = """
# Fill missing 'Model Year' with median (example)
if 'Model Year' in data.columns:
    median_year = data['Model Year'].median()
    data['Model Year'].fillna(median_year, inplace=True)
    # Ensure type conversion happens after filling NaNs
    if not data['Model Year'].isnull().any():
        data['Model Year'] = data['Model Year'].astype(int)

# Fill missing 'GVWR Class' after handling specific strings (example)
if 'GVWR Class' in data.columns:
    data['GVWR Class'] = data['GVWR Class'].replace({"Not Applicable": -1, "Unknown": -1})
    data['GVWR Class'] = pd.to_numeric(data['GVWR Class'], errors='coerce')
    # data['GVWR Class'].fillna(-1, inplace=True) # Optional: fill NaNs created by coerce

# Fill missing categoricals with mode (example)
for col in data.select_dtypes(include=['object', 'category']).columns:
     if data[col].isnull().any():
          mode_val = data[col].mode()[0] # Calculate mode safely
          data[col].fillna(mode_val, inplace=True)
"""
    st.code(code_step3, language="python")
    st.write("**Explanation**: Fill missing numerical with median, categorical with mode.")


    st.header("Step 4: Convert Data Types")
    # ... (code v√† gi·∫£i th√≠ch t∆∞∆°ng t·ª± nh∆∞ file g·ªëc) ...
    # L∆∞u √Ω: Vi·ªác chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu n√™n th·ª±c hi·ªán tr√™n data_processed
    code_step4 = """
# Convert 'Date' column (handled earlier with Vehicle Age calculation)
# Example: Convert other categoricals if needed after cleaning
categorical_columns_to_convert = [col for col in ['Vehicle Category', 'Fuel Type'] if col in data_processed.columns]
for col in categorical_columns_to_convert:
    data_processed[col] = data_processed[col].astype('category')
"""
    st.code(code_step4, language="python")
    st.write("**Explanation**: Convert relevant columns to appropriate types like 'category'.")

    st.header("Step 5: Remove Duplicates and Reset Index")
    # ... (code v√† gi·∫£i th√≠ch t∆∞∆°ng t·ª± nh∆∞ file g·ªëc) ...
    code_step5 = """
# Perform on the processed dataframe
rows_before = data_processed.shape[0]
data_processed.drop_duplicates(inplace=True)
data_processed.reset_index(drop=True, inplace=True)
rows_after = data_processed.shape[0]
print(f"Removed {rows_before - rows_after} duplicate rows.")
"""
    st.code(code_step5, language="python")
    st.write("**Explanation**: Remove duplicate rows and reset the index.")


    # Hi·ªÉn th·ªã d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω (t√πy ch·ªçn)
    st.write("### ‚ú® Processed Data Preview")
    st.dataframe(data_processed.head())

    # L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω ƒë·ªÉ c√°c trang kh√°c s·ª≠ d·ª•ng (QUAN TR·ªåNG)
    # C√≥ th·ªÉ l∆∞u v√†o session state ho·∫∑c l∆∞u ra file m·ªõi
    # V√≠ d·ª• l∆∞u v√†o session state:
    # st.session_state['processed_data'] = data_processed
    # V√≠ d·ª• l∆∞u ra file:
    # processed_file_path = os.path.join(DATA_DIR, 'processed_data.csv')
    # data_processed.to_csv(processed_file_path, index=False)
    # st.success(f"Processed data saved to {processed_file_path}")

else:
    st.error("Failed to load data for wrangling.")


# --- K·∫æT TH√öC CODE C·ª¶A PH·∫¶N data_wrangling ---